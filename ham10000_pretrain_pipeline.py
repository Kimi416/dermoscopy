"""
HAM10000ãƒ‡ãƒ¼ã‚¿ã§ã®äº‹å‰å­¦ç¿’ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
ISICç‰ˆã‹ã‚‰HAM10000ç‰ˆã¸ã®ç§»è¡Œ
"""

import os
import json
import pandas as pd
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.models import efficientnet_v2_s, resnet50
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
print(f"Using device: {device}")

class HAM10000Loader:
    """HAM10000ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, data_dir="ham10000_data"):
        self.data_dir = data_dir
        
        # åˆ†é¡ãƒãƒƒãƒ—èª­ã¿è¾¼ã¿
        map_file = os.path.join(data_dir, "binary_classification_map.json")
        if os.path.exists(map_file):
            with open(map_file, 'r', encoding='utf-8') as f:
                self.classification_map = json.load(f)
        else:
            print(f"âš ï¸ åˆ†é¡ãƒãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {map_file}")
            self.classification_map = self._create_default_map()
    
    def _create_default_map(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡ãƒãƒƒãƒ—ä½œæˆ"""
        return {
            'bkl': {'binary_label': 0, 'category': 'benign'},
            'df': {'binary_label': 0, 'category': 'benign'},
            'nv': {'binary_label': 0, 'category': 'benign'},
            'vasc': {'binary_label': 0, 'category': 'benign'},
            'akiec': {'binary_label': 1, 'category': 'malignant'},
            'bcc': {'binary_label': 1, 'category': 'malignant'},
            'mel': {'binary_label': 1, 'category': 'malignant'}
        }
    
    def load_metadata(self):
        """HAM10000ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        
        metadata_file = os.path.join(self.data_dir, "HAM10000_metadata.csv")
        
        if not os.path.exists(metadata_file):
            print(f"âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_file}")
            print("æ‰‹å‹•ã§HAM10000ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
            return None
        
        df = pd.read_csv(metadata_file)
        print(f"ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
        
        return df
    
    def prepare_binary_dataset(self, test_size=0.2):
        """HAM10000ã‚’è‰¯æ€§ãƒ»æ‚ªæ€§ã®2ã‚¯ãƒ©ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤‰æ›"""
        
        df = self.load_metadata()
        if df is None:
            return [], [], [], []
        
        image_paths = []
        labels = []
        
        # å„ç”»åƒã®ãƒ‘ã‚¹ã¨ãƒ©ãƒ™ãƒ«ã‚’æº–å‚™
        for _, row in df.iterrows():
            image_id = row['image_id']
            diagnosis = row['dx']
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆpart1ã¾ãŸã¯part2ã‹ã‚‰æ¤œç´¢ï¼‰
            image_file = f"{image_id}.jpg"
            image_path = None
            
            # part1, part2ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¤œç´¢
            for part_dir in ['HAM10000_images_part_1', 'HAM10000_images_part_2']:
                potential_path = os.path.join(self.data_dir, part_dir, image_file)
                if os.path.exists(potential_path):
                    image_path = potential_path
                    break
            
            if image_path and diagnosis in self.classification_map:
                image_paths.append(image_path)
                labels.append(self.classification_map[diagnosis]['binary_label'])
        
        print(f"ğŸ“Š æœ‰åŠ¹ãªç”»åƒæ•°: {len(image_paths)}")
        
        if len(image_paths) == 0:
            print("âŒ æœ‰åŠ¹ãªç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return [], [], [], []
        
        # å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            image_paths, labels, test_size=test_size, random_state=42, stratify=labels
        )
        
        # çµ±è¨ˆè¡¨ç¤º
        train_benign = sum(1 for label in y_train if label == 0)
        train_malignant = sum(1 for label in y_train if label == 1)
        test_benign = sum(1 for label in y_test if label == 0)
        test_malignant = sum(1 for label in y_test if label == 1)
        
        print(f"\\nğŸ“ˆ HAM10000ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²çµæœ:")
        print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)}æš (è‰¯æ€§: {train_benign}, æ‚ªæ€§: {train_malignant})")
        print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}æš (è‰¯æ€§: {test_benign}, æ‚ªæ€§: {test_malignant})")
        
        return X_train, X_test, y_train, y_test

class PretrainedModel(nn.Module):
    """äº‹å‰å­¦ç¿’ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆISICç‰ˆã¨åŒã˜ï¼‰"""
    
    def __init__(self, num_classes=2, model_type='efficientnet'):
        super().__init__()
        
        if model_type == 'efficientnet':
            self.backbone = efficientnet_v2_s(weights='IMAGENET1K_V1')
            num_features = self.backbone.classifier[1].in_features
            self.backbone.classifier = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(num_features, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
        else:  # resnet
            self.backbone = resnet50(weights='IMAGENET1K_V2')
            num_features = self.backbone.fc.in_features
            self.backbone.fc = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(num_features, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
    
    def forward(self, x):
        return self.backbone(x)

class DermoscopyDataset(Dataset):
    """ãƒ€ãƒ¼ãƒ¢ã‚¹ã‚³ãƒ”ãƒ¼ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆISICç‰ˆã¨åŒã˜ï¼‰"""
    
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            image = Image.new('RGB', (224, 224), (0, 0, 0))
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

def get_transforms(is_train=True):
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆISICç‰ˆã¨åŒã˜ï¼‰"""
    
    if is_train:
        return transforms.Compose([
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.RandomRotation(30),
            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    else:
        return transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

def pretrain_on_ham10000(model, train_loader, val_loader, epochs=10):
    """HAM10000ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’"""
    
    print("\\nğŸ”¬ HAM10000ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ä¸­...")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    best_val_acc = 0
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        # Validation
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'ham10000_pretrained_model.pth')
        
        scheduler.step()
    
    print(f"âœ… HAM10000äº‹å‰å­¦ç¿’å®Œäº†ã€‚æœ€é«˜ç²¾åº¦: {best_val_acc:.2f}%")
    return model

def load_user_data():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆISICç‰ˆã¨åŒã˜ã€SKã¯é™¤å¤–ï¼‰"""
    
    base_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢"
    
    # ç—…å¤‰ã‚¿ã‚¤ãƒ—åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆSKã¯é™¤å¤–ï¼‰
    disease_data = {
        'AK': {'paths': [], 'label': 1, 'name': 'æ—¥å…‰è§’åŒ–ç—‡'},
        'BCC': {'paths': [], 'label': 1, 'name': 'åŸºåº•ç´°èƒç™Œ'},
        'Bowenç—…': {'paths': [], 'label': 1, 'name': 'Bowenç—…'},
        'MM': {'paths': [], 'label': 1, 'name': 'æ‚ªæ€§é»’è‰²è…«'}
    }
    
    # å„ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç”»åƒãƒ‘ã‚¹ã‚’åé›†
    for folder_name, info in disease_data.items():
        folder_path = os.path.join(base_path, folder_name)
        if os.path.exists(folder_path):
            for img_file in os.listdir(folder_path):
                if img_file.endswith('.JPG'):
                    info['paths'].append(os.path.join(folder_path, img_file))
    
    # å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ï¼ˆå„ç–¾æ‚£ã‹ã‚‰20%ã‚’ãƒ†ã‚¹ãƒˆã«ï¼‰
    train_paths = []
    train_labels = []
    test_paths = []
    test_labels = []
    test_diseases = []
    
    for folder_name, info in disease_data.items():
        if len(info['paths']) > 0:
            disease_train, disease_test = train_test_split(
                info['paths'], test_size=0.2, random_state=42
            )
            
            train_paths.extend(disease_train)
            train_labels.extend([info['label']] * len(disease_train))
            
            test_paths.extend(disease_test)
            test_labels.extend([info['label']] * len(disease_test))
            test_diseases.extend([info['name']] * len(disease_test))
            
            print(f"{info['name']}: å­¦ç¿’ {len(disease_train)}æš, ãƒ†ã‚¹ãƒˆ {len(disease_test)}æš")
    
    return train_paths, train_labels, test_paths, test_labels, test_diseases

def load_sk_data_only():
    """SKãƒ‡ãƒ¼ã‚¿ï¼ˆè‰¯æ€§ï¼‰ã®ã¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆISICç‰ˆã¨åŒã˜ï¼‰"""
    
    base_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢"
    sk_folder = os.path.join(base_path, "SK")
    
    sk_paths = []
    sk_labels = []
    
    if os.path.exists(sk_folder):
        for img_file in os.listdir(sk_folder):
            if img_file.endswith('.JPG'):
                sk_paths.append(os.path.join(sk_folder, img_file))
                sk_labels.append(0)  # è‰¯æ€§
    
    print(f"SKï¼ˆè„‚æ¼æ€§è§’åŒ–ç—‡ï¼‰ãƒ‡ãƒ¼ã‚¿: {len(sk_paths)}æšï¼ˆè‰¯æ€§ï¼‰")
    
    # å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ï¼ˆ8:2ï¼‰
    if len(sk_paths) > 0:
        sk_train, sk_test, sk_train_labels, sk_test_labels = train_test_split(
            sk_paths, sk_labels, test_size=0.2, random_state=42
        )
        
        sk_test_diseases = ['è„‚æ¼æ€§è§’åŒ–ç—‡ï¼ˆè‰¯æ€§ï¼‰'] * len(sk_test)
        
        print(f"SKå­¦ç¿’ç”¨: {len(sk_train)}æš, ãƒ†ã‚¹ãƒˆç”¨: {len(sk_test)}æš")
        
        return sk_train, sk_train_labels, sk_test, sk_test_labels, sk_test_diseases
    else:
        return [], [], [], [], []

def finetune_on_user_data(model, train_loader, epochs=10):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆISICç‰ˆã¨åŒã˜ï¼‰"""
    
    print("\\nğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    
    params = [
        {'params': model.backbone.parameters(), 'lr': 1e-5},
    ]
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(params)
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%')
    
    torch.save(model.state_dict(), 'ham10000_finetuned_model.pth')
    print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
    return model

def finetune_with_sk_data(model, sk_train_loader, epochs=5):
    """SKãƒ‡ãƒ¼ã‚¿ï¼ˆè‰¯æ€§ï¼‰ã§3æ®µéšç›®ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
    
    print("\\nğŸŒŸ SKãƒ‡ãƒ¼ã‚¿ï¼ˆè‰¯æ€§ï¼‰ã§3æ®µéšç›®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)  # éå¸¸ã«ä½ã„å­¦ç¿’ç‡
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, labels in tqdm(sk_train_loader, desc=f'SK Epoch {epoch+1}/{epochs}'):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        print(f'SK Epoch {epoch+1}: Train Acc: {train_acc:.2f}%')
    
    torch.save(model.state_dict(), 'ham10000_balanced_finetuned_model.pth')
    print("âœ… HAM10000ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜")
    return model

def evaluate_on_test_data(model, test_loader, test_labels, test_diseases):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆISICç‰ˆã¨åŒã˜ï¼‰"""
    
    print("\\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...")
    
    model.eval()
    all_predictions = []
    all_probabilities = []
    
    with torch.no_grad():
        for images, _ in tqdm(test_loader, desc='è©•ä¾¡ä¸­'):
            images = images.to(device)
            outputs = model(images)
            probabilities = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_probabilities.extend(probabilities[:, 1].cpu().numpy())
    
    # å…¨ä½“ã®ç²¾åº¦
    accuracy = np.mean(np.array(all_predictions) == np.array(test_labels))
    print(f"\\nå…¨ä½“ç²¾åº¦: {accuracy:.2%}")
    
    # ç–¾æ‚£åˆ¥ã®ç²¾åº¦
    disease_results = {}
    unique_diseases = list(set(test_diseases))
    
    for disease in unique_diseases:
        disease_indices = [i for i, d in enumerate(test_diseases) if d == disease]
        disease_preds = [all_predictions[i] for i in disease_indices]
        disease_labels = [test_labels[i] for i in disease_indices]
        disease_acc = np.mean(np.array(disease_preds) == np.array(disease_labels))
        disease_results[disease] = {
            'accuracy': disease_acc,
            'total': len(disease_indices),
            'correct': sum(p == l for p, l in zip(disease_preds, disease_labels))
        }
    
    print("\\nç–¾æ‚£åˆ¥ç²¾åº¦:")
    for disease, results in disease_results.items():
        print(f"  {disease}: {results['accuracy']:.2%} ({results['correct']}/{results['total']})")
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(test_labels, all_predictions)
    
    # AUCè¨ˆç®—
    if len(set(test_labels)) == 2:
        auc = roc_auc_score(test_labels, all_probabilities)
        print(f"\\nAUC: {auc:.4f}")
    
    # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    print("\\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    unique_labels = sorted(list(set(test_labels)))
    if len(unique_labels) == 2:
        target_names = ['è‰¯æ€§', 'æ‚ªæ€§']
    else:
        target_names = [f'ã‚¯ãƒ©ã‚¹{i}' for i in unique_labels]
    
    print(classification_report(test_labels, all_predictions, 
                              target_names=target_names[:len(unique_labels)]))
    
    return cm, disease_results

def main():
    """HAM10000ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("=" * 60)
    print("ğŸ”¬ HAM10000ãƒ™ãƒ¼ã‚¹ 3æ®µéšãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("   Stage 1: ImageNet â†’ HAM10000")
    print("   Stage 2: HAM10000 â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ‚ªæ€§ãƒ‡ãƒ¼ã‚¿")
    print("   Stage 3: æ‚ªæ€§å­¦ç¿’æ¸ˆã¿ â†’ SKè‰¯æ€§ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ©ãƒ³ã‚¹èª¿æ•´")
    print("=" * 60)
    
    # 1. HAM10000ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    ham_loader = HAM10000Loader()
    X_train_ham, X_test_ham, y_train_ham, y_test_ham = ham_loader.prepare_binary_dataset()
    
    if len(X_train_ham) == 0:
        print("âŒ HAM10000ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        print("ãƒ‡ãƒ¢ç”¨ã«ISICãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™...")
        
        # ISICç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        from isic_pretrain_pipeline import main as isic_main
        isic_main()
        return
    
    # HAM10000ãƒ‡ãƒ¼ã‚¿ã®DataLoader
    train_dataset_ham = DermoscopyDataset(X_train_ham, y_train_ham, get_transforms(True))
    val_dataset_ham = DermoscopyDataset(X_test_ham, y_test_ham, get_transforms(False))
    
    train_loader_ham = DataLoader(train_dataset_ham, batch_size=32, shuffle=True, num_workers=2)
    val_loader_ham = DataLoader(val_dataset_ham, batch_size=32, shuffle=False, num_workers=2)
    
    # 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæ‚ªæ€§ã®ã¿ã€SKã¯é™¤å¤–ï¼‰
    user_train_paths, user_train_labels, user_test_paths, user_test_labels, test_diseases = load_user_data()
    
    print(f"\\nãƒ¦ãƒ¼ã‚¶ãƒ¼æ‚ªæ€§ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"  å­¦ç¿’ç”¨: {len(user_train_paths)}æš")
    print(f"  ãƒ†ã‚¹ãƒˆç”¨: {len(user_test_paths)}æš")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®DataLoader
    train_dataset_user = DermoscopyDataset(user_train_paths, user_train_labels, get_transforms(True))
    train_loader_user = DataLoader(train_dataset_user, batch_size=16, shuffle=True, num_workers=2)
    
    # 3. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = PretrainedModel(num_classes=2, model_type='efficientnet').to(device)
    
    # 4. Stage 1: HAM10000ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’
    model = pretrain_on_ham10000(model, train_loader_ham, val_loader_ham, epochs=5)
    
    # 5. Stage 2: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    if os.path.exists('ham10000_pretrained_model.pth'):
        model.load_state_dict(torch.load('ham10000_pretrained_model.pth', map_location=device))
        print("âœ… HAM10000ãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ")
    
    model = finetune_on_user_data(model, train_loader_user, epochs=10)
    
    # 6. Stage 3: SKãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
    sk_train_paths, sk_train_labels, sk_test_paths, sk_test_labels, sk_test_diseases = load_sk_data_only()
    
    if len(sk_train_paths) > 0:
        # SKãƒ‡ãƒ¼ã‚¿ã®DataLoader
        sk_train_dataset = DermoscopyDataset(sk_train_paths, sk_train_labels, get_transforms(True))
        sk_train_loader = DataLoader(sk_train_dataset, batch_size=16, shuffle=True, num_workers=2)
        
        # HAM10000ãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦SKã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
        if os.path.exists('ham10000_finetuned_model.pth'):
            model.load_state_dict(torch.load('ham10000_finetuned_model.pth', map_location=device))
            print("âœ… HAM10000ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿")
        
        model = finetune_with_sk_data(model, sk_train_loader, epochs=5)
        
        # 7. æ‹¡å¼µãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ‚ªæ€§ + SKè‰¯æ€§ï¼‰
        print("\\nğŸ“Š æ‹¡å¼µãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡...")
        
        combined_test_paths = user_test_paths + sk_test_paths
        combined_test_labels = user_test_labels + sk_test_labels
        combined_test_diseases = test_diseases + sk_test_diseases
        
        combined_test_dataset = DermoscopyDataset(combined_test_paths, combined_test_labels, get_transforms(False))
        combined_test_loader = DataLoader(combined_test_dataset, batch_size=16, shuffle=False, num_workers=2)
        
        cm, disease_results = evaluate_on_test_data(
            model, combined_test_loader, combined_test_labels, combined_test_diseases
        )
        
        print("\\nğŸ¯ HAM10000ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„åŠ¹æœ:")
        print(f"   æ‚ªæ€§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len([l for l in combined_test_labels if l == 1])}æš")
        print(f"   è‰¯æ€§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len([l for l in combined_test_labels if l == 0])}æš")
        
    else:
        print("âš ï¸ SKãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¾“æ¥ã®è©•ä¾¡ã‚’å®Ÿè¡Œ...")
        
        # å¾“æ¥ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è©•ä¾¡
        test_dataset_user = DermoscopyDataset(user_test_paths, user_test_labels, get_transforms(False))
        test_loader_user = DataLoader(test_dataset_user, batch_size=16, shuffle=False, num_workers=2)
        
        cm, disease_results = evaluate_on_test_data(
            model, test_loader_user, user_test_labels, test_diseases
        )
    
    print("\\nâœ… HAM10000ãƒ™ãƒ¼ã‚¹3æ®µéšãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
    print("\\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«:")
    print("   â€¢ ham10000_pretrained_model.pth: Stage 1å®Œäº†ï¼ˆHAM10000ã®ã¿ï¼‰")
    print("   â€¢ ham10000_finetuned_model.pth: Stage 2å®Œäº†ï¼ˆæ‚ªæ€§ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼‰")
    print("   â€¢ ham10000_balanced_finetuned_model.pth: Stage 3å®Œäº†ï¼ˆè‰¯æ€§ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰")

if __name__ == "__main__":
    main()