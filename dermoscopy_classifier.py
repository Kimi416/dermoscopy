"""
2æ®µéšãƒ€ãƒ¼ãƒ¢ã‚¹ã‚³ãƒ”ãƒ¼ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«
Stage 1: ç”»åƒå“è³ªæ”¹å–„ãƒ»å‰å‡¦ç†
Stage 2: è‰¯æ‚ªæ€§åˆ†é¡
"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from torchvision.models import swin_b, convnext_base
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
print(f"Using device: {device}")

class Stage1_ImageEnhancer:
    """Stage 1: ç”»åƒå“è³ªæ”¹å–„ãƒ»å‰å‡¦ç†"""
    
    def __init__(self):
        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    
    def remove_hair(self, img):
        """é«ªã®æ¯›é™¤å»ï¼ˆInpaintingãƒ™ãƒ¼ã‚¹ï¼‰"""
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        
        # Black Hat Transform ã§é«ªã®æ¯›æ¤œå‡º
        blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, self.kernel)
        
        # é–¾å€¤å‡¦ç†ã§ãƒã‚¹ã‚¯ä½œæˆ
        _, mask = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)
        
        # Inpainting ã§é«ªã®æ¯›ã‚’é™¤å»
        result = cv2.inpaint(img, mask, 1, cv2.INPAINT_TELEA)
        return result
    
    def enhance_contrast(self, img):
        """ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·èª¿ï¼ˆCLAHEï¼‰"""
        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
        l, a, b = cv2.split(lab)
        
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        
        enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)
        return enhanced
    
    def normalize_color(self, img):
        """è‰²å½©æ­£è¦åŒ–"""
        # å„ãƒãƒ£ãƒ³ãƒãƒ«ã®æ­£è¦åŒ–
        normalized = np.zeros_like(img, dtype=np.float32)
        for i in range(3):
            channel = img[:, :, i].astype(np.float32)
            mean = np.mean(channel)
            std = np.std(channel)
            normalized[:, :, i] = (channel - mean) / (std + 1e-8)
        
        # 0-255ã®ç¯„å›²ã«æˆ»ã™
        normalized = (normalized - normalized.min()) / (normalized.max() - normalized.min() + 1e-8)
        normalized = (normalized * 255).astype(np.uint8)
        return normalized
    
    def process(self, img):
        """å…¨å‰å‡¦ç†ã‚’é©ç”¨"""
        img = self.remove_hair(img)
        img = self.enhance_contrast(img)
        img = self.normalize_color(img)
        return img

class DermoscopyDataset(Dataset):
    """ãƒ€ãƒ¼ãƒ¢ã‚¹ã‚³ãƒ”ãƒ¼ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, image_paths, labels, transform=None, stage1_enhancer=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.stage1_enhancer = stage1_enhancer
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        
        # ç”»åƒèª­ã¿è¾¼ã¿
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Stage 1: å‰å‡¦ç†
        if self.stage1_enhancer:
            image = self.stage1_enhancer.process(image)
        
        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label

class ClinicalFeatureExtractor(nn.Module):
    """è‡¨åºŠçš„ç‰¹å¾´ï¼ˆABCDåŸºæº–ï¼‰ã®è‡ªå‹•æŠ½å‡º"""
    
    def __init__(self):
        super().__init__()
        # Asymmetryï¼ˆéå¯¾ç§°æ€§ï¼‰æ¤œå‡º
        self.asymmetry_conv = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 3, padding=1),
            nn.AdaptiveAvgPool2d(1)
        )
        
        # Borderï¼ˆå¢ƒç•Œï¼‰ä¸æ•´æ¤œå‡º
        self.border_conv = nn.Sequential(
            nn.Conv2d(3, 32, 5, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 5, padding=2),
            nn.AdaptiveAvgPool2d(1)
        )
        
        # Colorï¼ˆè‰²å½©ï¼‰å¤šæ§˜æ€§æ¤œå‡º
        self.color_conv = nn.Sequential(
            nn.Conv2d(3, 64, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 32, 1),
            nn.AdaptiveAvgPool2d(1)
        )
        
        # Diameter/Differential structuresï¼ˆæ§‹é€ ï¼‰æ¤œå‡º
        self.structure_conv = nn.Sequential(
            nn.Conv2d(3, 32, 7, padding=3),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 7, padding=3),
            nn.AdaptiveAvgPool2d(1)
        )
    
    def forward(self, x):
        asymmetry = self.asymmetry_conv(x).flatten(1)
        border = self.border_conv(x).flatten(1)
        color = self.color_conv(x).flatten(1)
        structure = self.structure_conv(x).flatten(1)
        
        # ABCDç‰¹å¾´ã‚’çµåˆ
        clinical_features = torch.cat([asymmetry, border, color, structure], dim=1)
        return clinical_features

class HierarchicalClassifier(nn.Module):
    """éšå±¤çš„åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆé«˜ç²¾åº¦ç‰ˆï¼‰"""
    
    def __init__(self, num_classes=9):
        super().__init__()
        
        # è‡¨åºŠçš„ç‰¹å¾´æŠ½å‡º
        self.clinical_extractor = ClinicalFeatureExtractor()
        
        # Swin Transformerï¼ˆæ·±å±¤ç‰¹å¾´ï¼‰
        self.swin = swin_b(weights='IMAGENET1K_V1')
        self.swin.head = nn.Linear(self.swin.head.in_features, 512)
        
        # ConvNeXtï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ç‰¹å¾´ï¼‰
        self.convnext = convnext_base(weights='IMAGENET1K_V1')
        self.convnext.classifier[2] = nn.Linear(self.convnext.classifier[2].in_features, 512)
        
        # Attentionæ©Ÿæ§‹
        self.attention = nn.MultiheadAttention(embed_dim=1024, num_heads=8)
        
        # Stage 1: è‰¯æ€§/æ‚ªæ€§åˆ†é¡
        self.malignancy_classifier = nn.Sequential(
            nn.Linear(1024 + 80, 512),  # æ·±å±¤ç‰¹å¾´ + ABCDç‰¹å¾´
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2)  # è‰¯æ€§/æ‚ªæ€§
        )
        
        # Stage 2: è©³ç´°åˆ†é¡
        self.detail_classifier = nn.Sequential(
            nn.Linear(1024 + 80 + 2, 512),  # ç‰¹å¾´ + è‰¯æ‚ªæ€§ç¢ºç‡
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # è‡¨åºŠçš„ç‰¹å¾´æŠ½å‡º
        clinical_features = self.clinical_extractor(x)
        
        # æ·±å±¤ç‰¹å¾´æŠ½å‡º
        swin_features = self.swin(x)
        convnext_features = self.convnext(x)
        deep_features = torch.cat([swin_features, convnext_features], dim=1)
        
        # Self-Attention
        deep_features_reshaped = deep_features.unsqueeze(0)
        attended_features, _ = self.attention(
            deep_features_reshaped, 
            deep_features_reshaped, 
            deep_features_reshaped
        )
        attended_features = attended_features.squeeze(0)
        
        # å…¨ç‰¹å¾´ã‚’çµåˆ
        all_features = torch.cat([attended_features, clinical_features], dim=1)
        
        # Stage 1: è‰¯æ‚ªæ€§åˆ¤å®š
        malignancy_logits = self.malignancy_classifier(all_features)
        malignancy_probs = torch.softmax(malignancy_logits, dim=1)
        
        # Stage 2: è©³ç´°åˆ†é¡ï¼ˆè‰¯æ‚ªæ€§ç¢ºç‡ã‚‚å…¥åŠ›ï¼‰
        combined_input = torch.cat([all_features, malignancy_probs], dim=1)
        detail_logits = self.detail_classifier(combined_input)
        
        return {
            'malignancy': malignancy_logits,
            'detail': detail_logits,
            'clinical_features': clinical_features
        }

class Stage2_Classifier(nn.Module):
    """Stage 2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, num_classes=2):
        super().__init__()
        
        # Swin Transformer
        self.swin = swin_b(weights='IMAGENET1K_V1')
        self.swin.head = nn.Linear(self.swin.head.in_features, 256)
        
        # ConvNeXt
        self.convnext = convnext_base(weights='IMAGENET1K_V1')
        self.convnext.classifier[2] = nn.Linear(self.convnext.classifier[2].in_features, 256)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å±¤
        self.fusion = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        # ä¸¡ãƒ¢ãƒ‡ãƒ«ã§ç‰¹å¾´æŠ½å‡º
        swin_features = self.swin(x)
        convnext_features = self.convnext(x)
        
        # ç‰¹å¾´ã‚’çµåˆ
        combined = torch.cat([swin_features, convnext_features], dim=1)
        
        # æœ€çµ‚åˆ†é¡
        output = self.fusion(combined)
        return output

def get_augmentation_pipeline(is_train=True):
    """ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    if is_train:
        return A.Compose([
            A.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.RandomRotate90(p=0.5),
            A.OneOf([
                A.GaussNoise(p=1),
                A.GaussianBlur(p=1),
                A.MotionBlur(p=1),
            ], p=0.3),
            A.OneOf([
                A.OpticalDistortion(p=1),
                A.GridDistortion(p=1),
                A.ElasticTransform(p=1),
            ], p=0.3),
            A.OneOf([
                A.HueSaturationValue(p=1),
                A.RandomBrightnessContrast(p=1),
                A.ColorJitter(p=1),
            ], p=0.5),
            A.CoarseDropout(max_holes=8, max_height=20, max_width=20, p=0.3),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.Resize(height=224, width=224),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])

def load_dataset(use_detailed_labels=False):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    
    Args:
        use_detailed_labels: True ã§è©³ç´°ãªç—…å¤‰åˆ†é¡ã€False ã§è‰¯æ‚ªæ€§ã®ã¿
    """
    
    base_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢"
    
    # ç—…å¤‰ã‚¯ãƒ©ã‚¹ã®å®šç¾©
    class_mapping = {
        # æ‚ªæ€§ç—…å¤‰
        'AK': {'malignant': 1, 'detail': 0, 'name': 'æ—¥å…‰è§’åŒ–ç—‡'},
        'BCC': {'malignant': 1, 'detail': 1, 'name': 'åŸºåº•ç´°èƒç™Œ'},
        'Bowenç—…': {'malignant': 1, 'detail': 2, 'name': 'Bowenç—…'},
        'MM': {'malignant': 1, 'detail': 3, 'name': 'æ‚ªæ€§é»’è‰²è…«'},
        
        # è‰¯æ€§ç—…å¤‰
        'SK': {'malignant': 0, 'detail': 4, 'name': 'è„‚æ¼æ€§è§’åŒ–ç—‡'},
        'nevus': {'malignant': 0, 'detail': 5, 'name': 'æ¯æ–‘'},
        'DF': {'malignant': 0, 'detail': 6, 'name': 'çš®è†šç·šç¶­è…«'},
        'VASC': {'malignant': 0, 'detail': 7, 'name': 'è¡€ç®¡ç—…å¤‰'},
        'benign': {'malignant': 0, 'detail': 8, 'name': 'ãã®ä»–è‰¯æ€§'}
    }
    
    image_paths = []
    labels = []
    detailed_labels = []
    class_counts = {}
    
    # å„ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç”»åƒã‚’èª­ã¿è¾¼ã¿
    for folder_name, class_info in class_mapping.items():
        folder_path = os.path.join(base_path, folder_name)
        if os.path.exists(folder_path):
            count = 0
            for img_file in os.listdir(folder_path):
                if img_file.endswith(('.JPG', '.jpg', '.png')):
                    image_paths.append(os.path.join(folder_path, img_file))
                    labels.append(class_info['malignant'])
                    detailed_labels.append(class_info['detail'])
                    count += 1
            if count > 0:
                class_counts[class_info['name']] = count
                print(f"{class_info['name']}: {count}æš")
    
    print(f"\nåˆè¨ˆç”»åƒæ•°: {len(image_paths)}")
    print(f"æ‚ªæ€§: {sum(1 for l in labels if l == 1)}æš")
    print(f"è‰¯æ€§: {sum(1 for l in labels if l == 0)}æš")
    
    if use_detailed_labels:
        return image_paths, detailed_labels, class_mapping
    else:
        return image_paths, labels

def train_model(model, train_loader, val_loader, num_epochs=30):
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    best_val_acc = 0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Train'):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        # Validation
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Val'):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        # çµ±è¨ˆè¨˜éŒ²
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        history['train_loss'].append(train_loss / len(train_loader))
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss / len(val_loader))
        history['val_acc'].append(val_acc)
        
        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_dermoscopy_model.pth')
            print(f'Best model saved with accuracy: {val_acc:.2f}%')
        
        scheduler.step()
    
    return history

def visualize_results(history):
    """å­¦ç¿’çµæœã®å¯è¦–åŒ–"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Loss
    ax1.plot(history['train_loss'], label='Train Loss')
    ax1.plot(history['val_loss'], label='Val Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()
    ax1.grid(True)
    
    # Accuracy
    ax2.plot(history['train_acc'], label='Train Acc')
    ax2.plot(history['val_acc'], label='Val Acc')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Training and Validation Accuracy')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png')
    plt.show()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ”¬ 2æ®µéšãƒ€ãƒ¼ãƒ¢ã‚¹ã‚³ãƒ”ãƒ¼åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    image_paths, labels = load_dataset()
    
    if len(set(labels)) < 2:
        print("\nâš ï¸ ã‚¨ãƒ©ãƒ¼: è‰¯æ€§ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
        print("ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§è‰¯æ€§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼š")
        print("1. ISICã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        print("2. 'benign'ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦è‰¯æ€§ç”»åƒã‚’é…ç½®")
        return
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_val, y_train, y_val = train_test_split(
        image_paths, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # Stage 1: å‰å‡¦ç†å™¨
    stage1_enhancer = Stage1_ImageEnhancer()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_dataset = DermoscopyDataset(
        X_train, y_train,
        transform=get_augmentation_pipeline(is_train=True),
        stage1_enhancer=stage1_enhancer
    )
    
    val_dataset = DermoscopyDataset(
        X_val, y_val,
        transform=get_augmentation_pipeline(is_train=False),
        stage1_enhancer=stage1_enhancer
    )
    
    # DataLoader
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)
    
    # Stage 2: åˆ†é¡ãƒ¢ãƒ‡ãƒ«
    model = Stage2_Classifier(num_classes=2).to(device)
    
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:")
    print(f"Training: {len(train_dataset)} images")
    print(f"Validation: {len(val_dataset)} images")
    print(f"Device: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    print("\nğŸš€ å­¦ç¿’é–‹å§‹...")
    history = train_model(model, train_loader, val_loader, num_epochs=5)
    
    # çµæœå¯è¦–åŒ–
    visualize_results(history)
    
    print("\nâœ… å­¦ç¿’å®Œäº†!")
    print("ãƒ¢ãƒ‡ãƒ«ã¯ 'best_dermoscopy_model.pth' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()