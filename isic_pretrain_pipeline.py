"""
ISICãƒ‡ãƒ¼ã‚¿ã§ã®äº‹å‰å­¦ç¿’ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import os
import json
import requests
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.models import efficientnet_v2_s, resnet50
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image
from io import BytesIO
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time
import shutil
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
print(f"Using device: {device}")

class ISICDownloader:
    """ISICã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆv2 APIä½¿ç”¨ï¼‰"""
    
    def __init__(self, output_dir="isic_data"):
        self.output_dir = output_dir
        self.api_base = "https://api.isic-archive.com/api/v2"
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(f"{output_dir}/benign", exist_ok=True)
        os.makedirs(f"{output_dir}/malignant", exist_ok=True)
    
    def download_images(self, benign_count=1000, malignant_count=1000):
        """è‰¯æ€§ãƒ»æ‚ªæ€§ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        
        print("ğŸ“¥ ISIC v2 APIã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        # è‰¯æ€§ç”»åƒï¼ˆæ¯æ–‘ã€è„‚æ¼æ€§è§’åŒ–ç—‡ãªã©ï¼‰
        benign_downloaded = self._download_by_diagnosis(
            ["Nevus", "Solar lentigo", "Seborrheic keratosis"], 
            "benign", 
            benign_count
        )
        
        # æ‚ªæ€§ç”»åƒï¼ˆãƒ¡ãƒ©ãƒãƒ¼ãƒã€åŸºåº•ç´°èƒç™Œãªã©ï¼‰
        malignant_downloaded = self._download_by_diagnosis(
            ["Melanoma", "Basal cell carcinoma", "Squamous cell carcinoma"], 
            "malignant", 
            malignant_count
        )
        
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: è‰¯æ€§ {benign_downloaded}æš, æ‚ªæ€§ {malignant_downloaded}æš")
    
    def _download_by_diagnosis(self, diagnoses, category, target_count):
        """è¨ºæ–­ååˆ¥ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆv2 APIä½¿ç”¨ï¼‰"""
        
        downloaded = 0
        limit = 50
        cursor = None
        
        with tqdm(total=target_count, desc=f"{category}ç”»åƒ") as pbar:
            
            while downloaded < target_count:
                try:
                    # API ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                    params = {"limit": limit}
                    if cursor:
                        params["cursor"] = cursor
                    
                    response = requests.get(
                        f"{self.api_base}/images/",
                        params=params,
                        timeout=30
                    )
                    
                    if response.status_code != 200:
                        print(f"API Error: {response.status_code}")
                        break
                    
                    data = response.json()
                    results = data.get("results", [])
                    
                    if not results:
                        break
                    
                    # å„ç”»åƒã‚’å‡¦ç†
                    for item in results:
                        if downloaded >= target_count:
                            break
                        
                        # è¨ºæ–­åã‚’ãƒã‚§ãƒƒã‚¯
                        metadata = item.get("metadata", {})
                        clinical = metadata.get("clinical", {})
                        
                        # è¨ºæ–­åã®éšå±¤ã‚’ãƒã‚§ãƒƒã‚¯
                        diagnosis_found = False
                        for diag_key in ["diagnosis_1", "diagnosis_2", "diagnosis_3", "diagnosis_4", "diagnosis_5"]:
                            diagnosis = clinical.get(diag_key, "")
                            if any(target_diag.lower() in diagnosis.lower() for target_diag in diagnoses):
                                diagnosis_found = True
                                break
                        
                        if not diagnosis_found:
                            continue
                        
                        # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        isic_id = item.get("isic_id")
                        if not isic_id:
                            continue
                        
                        img_path = f"{self.output_dir}/{category}/{isic_id}.jpg"
                        
                        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if os.path.exists(img_path):
                            downloaded += 1
                            pbar.update(1)
                            continue
                        
                        # ç”»åƒURLå–å¾—
                        files = item.get("files", {})
                        full_img = files.get("full", {})
                        img_url = full_img.get("url")
                        
                        if not img_url:
                            continue
                        
                        # ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        img_response = requests.get(img_url, stream=True, timeout=30)
                        
                        if img_response.status_code == 200:
                            try:
                                img = Image.open(BytesIO(img_response.content))
                                # ãƒªã‚µã‚¤ã‚ºã—ã¦ä¿å­˜
                                img.thumbnail((512, 512), Image.Resampling.LANCZOS)
                                img.save(img_path, "JPEG", quality=95)
                                downloaded += 1
                                pbar.update(1)
                            except Exception as e:
                                print(f"ç”»åƒä¿å­˜ã‚¨ãƒ©ãƒ¼ {isic_id}: {e}")
                        
                        time.sleep(0.1)  # APIåˆ¶é™å¯¾ç­–
                    
                    # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
                    next_url = data.get("next")
                    if next_url and "cursor=" in next_url:
                        cursor = next_url.split("cursor=")[1].split("&")[0]
                    else:
                        break
                
                except Exception as e:
                    print(f"\nã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(5)
                    continue
        
        return downloaded

class PretrainedModel(nn.Module):
    """äº‹å‰å­¦ç¿’ç”¨ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, num_classes=2, model_type='efficientnet'):
        super().__init__()
        
        if model_type == 'efficientnet':
            self.backbone = efficientnet_v2_s(weights='IMAGENET1K_V1')
            num_features = self.backbone.classifier[1].in_features
            self.backbone.classifier = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(num_features, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
        else:  # resnet
            self.backbone = resnet50(weights='IMAGENET1K_V2')
            num_features = self.backbone.fc.in_features
            self.backbone.fc = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(num_features, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
    
    def forward(self, x):
        return self.backbone(x)

class DermoscopyDataset(Dataset):
    """ãƒ€ãƒ¼ãƒ¢ã‚¹ã‚³ãƒ”ãƒ¼ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            # ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼æ™‚ã¯é»’ç”»åƒã‚’è¿”ã™
            image = Image.new('RGB', (224, 224), (0, 0, 0))
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

def get_transforms(is_train=True):
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    if is_train:
        return transforms.Compose([
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.RandomRotation(30),
            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    else:
        return transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

def load_isic_data():
    """ISICãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    
    isic_dir = "isic_data"
    image_paths = []
    labels = []
    
    # è‰¯æ€§ç”»åƒ
    benign_dir = f"{isic_dir}/benign"
    if os.path.exists(benign_dir):
        for img_file in os.listdir(benign_dir):
            if img_file.endswith('.jpg'):
                image_paths.append(os.path.join(benign_dir, img_file))
                labels.append(0)
    
    # æ‚ªæ€§ç”»åƒ
    malignant_dir = f"{isic_dir}/malignant"
    if os.path.exists(malignant_dir):
        for img_file in os.listdir(malignant_dir):
            if img_file.endswith('.jpg'):
                image_paths.append(os.path.join(malignant_dir, img_file))
                labels.append(1)
    
    print(f"ISICãƒ‡ãƒ¼ã‚¿: è‰¯æ€§ {labels.count(0)}æš, æ‚ªæ€§ {labels.count(1)}æš")
    return image_paths, labels

def load_user_data():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰²"""
    
    base_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢"
    
    # ç—…å¤‰ã‚¿ã‚¤ãƒ—åˆ¥ã®ãƒ‡ãƒ¼ã‚¿
    disease_data = {
        'AK': {'paths': [], 'label': 1, 'name': 'æ—¥å…‰è§’åŒ–ç—‡'},
        'BCC': {'paths': [], 'label': 1, 'name': 'åŸºåº•ç´°èƒç™Œ'},
        'Bowenç—…': {'paths': [], 'label': 1, 'name': 'Bowenç—…'},
        'MM': {'paths': [], 'label': 1, 'name': 'æ‚ªæ€§é»’è‰²è…«'},
        'SK': {'paths': [], 'label': 0, 'name': 'è„‚æ¼æ€§è§’åŒ–ç—‡ï¼ˆè‰¯æ€§ï¼‰'}
    }
    
    # å„ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ç”»åƒãƒ‘ã‚¹ã‚’åé›†
    for folder_name, info in disease_data.items():
        folder_path = os.path.join(base_path, folder_name)
        if os.path.exists(folder_path):
            for img_file in os.listdir(folder_path):
                if img_file.endswith('.JPG'):
                    info['paths'].append(os.path.join(folder_path, img_file))
    
    # å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ï¼ˆå„ç–¾æ‚£ã‹ã‚‰20%ã‚’ãƒ†ã‚¹ãƒˆã«ï¼‰
    train_paths = []
    train_labels = []
    test_paths = []
    test_labels = []
    test_diseases = []  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç–¾æ‚£å
    
    for folder_name, info in disease_data.items():
        if len(info['paths']) > 0:
            # å„ç–¾æ‚£ã‚’8:2ã§åˆ†å‰²
            disease_train, disease_test = train_test_split(
                info['paths'], test_size=0.2, random_state=42
            )
            
            train_paths.extend(disease_train)
            train_labels.extend([info['label']] * len(disease_train))
            
            test_paths.extend(disease_test)
            test_labels.extend([info['label']] * len(disease_test))
            test_diseases.extend([info['name']] * len(disease_test))
            
            print(f"{info['name']}: å­¦ç¿’ {len(disease_train)}æš, ãƒ†ã‚¹ãƒˆ {len(disease_test)}æš")
    
    return train_paths, train_labels, test_paths, test_labels, test_diseases

def load_sk_data_only():
    """SKãƒ‡ãƒ¼ã‚¿ï¼ˆè‰¯æ€§ï¼‰ã®ã¿ã‚’èª­ã¿è¾¼ã¿"""
    
    base_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢"
    sk_folder = os.path.join(base_path, "SK")
    
    sk_paths = []
    sk_labels = []
    
    if os.path.exists(sk_folder):
        for img_file in os.listdir(sk_folder):
            if img_file.endswith('.JPG'):
                sk_paths.append(os.path.join(sk_folder, img_file))
                sk_labels.append(0)  # è‰¯æ€§
    
    print(f"SKï¼ˆè„‚æ¼æ€§è§’åŒ–ç—‡ï¼‰ãƒ‡ãƒ¼ã‚¿: {len(sk_paths)}æšï¼ˆè‰¯æ€§ï¼‰")
    
    # å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ï¼ˆ8:2ï¼‰
    if len(sk_paths) > 0:
        sk_train, sk_test, sk_train_labels, sk_test_labels = train_test_split(
            sk_paths, sk_labels, test_size=0.2, random_state=42
        )
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ç–¾æ‚£å
        sk_test_diseases = ['è„‚æ¼æ€§è§’åŒ–ç—‡ï¼ˆè‰¯æ€§ï¼‰'] * len(sk_test)
        
        print(f"SKå­¦ç¿’ç”¨: {len(sk_train)}æš, ãƒ†ã‚¹ãƒˆç”¨: {len(sk_test)}æš")
        
        return sk_train, sk_train_labels, sk_test, sk_test_labels, sk_test_diseases
    else:
        return [], [], [], [], []

def pretrain_on_isic(model, train_loader, val_loader, epochs=10):
    """ISICãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’"""
    
    print("\nğŸ”¬ ISICãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ä¸­...")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    best_val_acc = 0
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        # Validation
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'isic_pretrained_model.pth')
        
        scheduler.step()
    
    print(f"âœ… äº‹å‰å­¦ç¿’å®Œäº†ã€‚æœ€é«˜ç²¾åº¦: {best_val_acc:.2f}%")
    return model

def finetune_on_user_data(model, train_loader, val_loader, epochs=10):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
    
    print("\nğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    
    # æœ€å¾Œã®å±¤ã®ã¿å­¦ç¿’ç‡ã‚’é«˜ãè¨­å®š
    params = [
        {'params': model.backbone.parameters(), 'lr': 1e-5},
    ]
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(params)
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%')
    
    torch.save(model.state_dict(), 'finetuned_model.pth')
    print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
    return model

def evaluate_on_test_data(model, test_loader, test_labels, test_diseases):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡"""
    
    print("\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...")
    
    model.eval()
    all_predictions = []
    all_probabilities = []
    
    with torch.no_grad():
        for images, _ in tqdm(test_loader, desc='è©•ä¾¡ä¸­'):
            images = images.to(device)
            outputs = model(images)
            probabilities = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # æ‚ªæ€§ã®ç¢ºç‡
    
    # å…¨ä½“ã®ç²¾åº¦
    accuracy = np.mean(np.array(all_predictions) == np.array(test_labels))
    print(f"\nå…¨ä½“ç²¾åº¦: {accuracy:.2%}")
    
    # ç–¾æ‚£åˆ¥ã®ç²¾åº¦
    disease_results = {}
    unique_diseases = list(set(test_diseases))
    
    for disease in unique_diseases:
        disease_indices = [i for i, d in enumerate(test_diseases) if d == disease]
        disease_preds = [all_predictions[i] for i in disease_indices]
        disease_labels = [test_labels[i] for i in disease_indices]
        disease_acc = np.mean(np.array(disease_preds) == np.array(disease_labels))
        disease_results[disease] = {
            'accuracy': disease_acc,
            'total': len(disease_indices),
            'correct': sum(p == l for p, l in zip(disease_preds, disease_labels))
        }
    
    print("\nç–¾æ‚£åˆ¥ç²¾åº¦:")
    for disease, results in disease_results.items():
        print(f"  {disease}: {results['accuracy']:.2%} ({results['correct']}/{results['total']})")
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(test_labels, all_predictions)
    
    # AUCè¨ˆç®—
    if len(set(test_labels)) == 2:
        auc = roc_auc_score(test_labels, all_probabilities)
        print(f"\nAUC: {auc:.4f}")
    
    # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    unique_labels = sorted(list(set(test_labels)))
    if len(unique_labels) == 2:
        target_names = ['è‰¯æ€§', 'æ‚ªæ€§']
    else:
        target_names = [f'ã‚¯ãƒ©ã‚¹{i}' for i in unique_labels]
    
    print(classification_report(test_labels, all_predictions, 
                              target_names=target_names[:len(unique_labels)]))
    
    return cm, disease_results

def plot_results(cm, disease_results):
    """çµæœã®å¯è¦–åŒ–"""
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # æ··åŒè¡Œåˆ—
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['è‰¯æ€§', 'æ‚ªæ€§'],
                yticklabels=['è‰¯æ€§', 'æ‚ªæ€§'],
                ax=axes[0])
    axes[0].set_title('æ··åŒè¡Œåˆ—')
    axes[0].set_xlabel('äºˆæ¸¬')
    axes[0].set_ylabel('å®Ÿéš›')
    
    # ç–¾æ‚£åˆ¥ç²¾åº¦
    diseases = list(disease_results.keys())
    accuracies = [disease_results[d]['accuracy'] for d in diseases]
    
    axes[1].barh(diseases, accuracies)
    axes[1].set_xlabel('ç²¾åº¦')
    axes[1].set_title('ç–¾æ‚£åˆ¥ç²¾åº¦')
    axes[1].set_xlim([0, 1])
    
    for i, (disease, acc) in enumerate(zip(diseases, accuracies)):
        axes[1].text(acc + 0.01, i, f'{acc:.1%}', va='center')
    
    plt.tight_layout()
    plt.savefig('evaluation_results.png', dpi=150)
    plt.show()
    
    print("\nğŸ“ˆ çµæœã‚’ 'evaluation_results.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")

def finetune_with_sk_data(model, sk_train_loader, epochs=5):
    """SKãƒ‡ãƒ¼ã‚¿ï¼ˆè‰¯æ€§ï¼‰ã§3æ®µéšç›®ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
    
    print("\nğŸŒŸ SKãƒ‡ãƒ¼ã‚¿ï¼ˆè‰¯æ€§ï¼‰ã§3æ®µéšç›®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    
    # éå¸¸ã«ä½ã„å­¦ç¿’ç‡ã§æ—¢å­˜çŸ¥è­˜ã‚’ä¿æŒ
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)  # éå¸¸ã«ä½ã„å­¦ç¿’ç‡
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for images, labels in tqdm(sk_train_loader, desc=f'SK Epoch {epoch+1}/{epochs}'):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        print(f'SK Epoch {epoch+1}: Train Acc: {train_acc:.2f}%')
    
    # ãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    torch.save(model.state_dict(), 'balanced_finetuned_model.pth')
    print("âœ… ãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ 'balanced_finetuned_model.pth' ã«ä¿å­˜")
    return model

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ3æ®µéšãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼‰"""
    
    print("=" * 60)
    print("ğŸ”¬ 3æ®µéšãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("   Stage 1: ImageNet â†’ ISIC")
    print("   Stage 2: ISIC â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ‚ªæ€§ãƒ‡ãƒ¼ã‚¿")
    print("   Stage 3: æ‚ªæ€§å­¦ç¿’æ¸ˆã¿ â†’ SKè‰¯æ€§ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ©ãƒ³ã‚¹èª¿æ•´")
    print("=" * 60)
    
    # 1. ISICãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    downloader = ISICDownloader()
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
    isic_paths, isic_labels = load_isic_data()
    
    if len(isic_paths) < 200:
        print("ISICãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")
        downloader.download_images(benign_count=100, malignant_count=100)
        isic_paths, isic_labels = load_isic_data()
    
    if len(isic_paths) == 0:
        print("âš ï¸ ISICãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        print("ä»£æ›¿æ¡ˆ: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ã—ã¾ã™ã€‚")
        
    # 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    # ISICãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    if len(isic_paths) > 0:
        X_train_isic, X_val_isic, y_train_isic, y_val_isic = train_test_split(
            isic_paths, isic_labels, test_size=0.2, random_state=42, stratify=isic_labels
        )
        
        train_dataset_isic = DermoscopyDataset(X_train_isic, y_train_isic, get_transforms(True))
        val_dataset_isic = DermoscopyDataset(X_val_isic, y_val_isic, get_transforms(False))
        
        train_loader_isic = DataLoader(train_dataset_isic, batch_size=32, shuffle=True, num_workers=2)
        val_loader_isic = DataLoader(val_dataset_isic, batch_size=32, shuffle=False, num_workers=2)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæ‚ªæ€§ã®ã¿ã€SKã¯é™¤å¤–ï¼‰
    user_train_paths, user_train_labels, user_test_paths, user_test_labels, test_diseases = load_user_data()
    
    # SKãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å†æ§‹æˆ
    filtered_train_paths = []
    filtered_train_labels = []
    filtered_test_paths = []
    filtered_test_labels = []
    filtered_test_diseases = []
    
    for i, path in enumerate(user_train_paths):
        if 'SK' not in path:
            filtered_train_paths.append(path)
            filtered_train_labels.append(user_train_labels[i])
    
    for i, path in enumerate(user_test_paths):
        if 'SK' not in path:
            filtered_test_paths.append(path)
            filtered_test_labels.append(user_test_labels[i])
            filtered_test_diseases.append(test_diseases[i])
    
    user_train_paths = filtered_train_paths
    user_train_labels = filtered_train_labels
    user_test_paths = filtered_test_paths
    user_test_labels = filtered_test_labels
    test_diseases = filtered_test_diseases
    
    print(f"\næ‚ªæ€§ãƒ‡ãƒ¼ã‚¿ã®ã¿çµ±è¨ˆ:")
    print(f"  å­¦ç¿’ç”¨: {len(user_train_paths)}æš")
    print(f"  ãƒ†ã‚¹ãƒˆç”¨: {len(user_test_paths)}æš")
    
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®DataLoader
    train_dataset_user = DermoscopyDataset(user_train_paths, user_train_labels, get_transforms(True))
    test_dataset_user = DermoscopyDataset(user_test_paths, user_test_labels, get_transforms(False))
    
    train_loader_user = DataLoader(train_dataset_user, batch_size=16, shuffle=True, num_workers=2)
    test_loader_user = DataLoader(test_dataset_user, batch_size=16, shuffle=False, num_workers=2)
    
    # 3. æ—¢å­˜ã®finetuned_modelã‚’èª­ã¿è¾¼ã¿ï¼ˆStage 2å®Œäº†æ¸ˆã¿ï¼‰
    print("\nğŸ”„ æ—¢å­˜ã®finetuned_model.pthã‚’èª­ã¿è¾¼ã¿...")
    model = PretrainedModel(num_classes=2, model_type='efficientnet').to(device)
    
    if os.path.exists('finetuned_model.pth'):
        model.load_state_dict(torch.load('finetuned_model.pth', map_location=device))
        print("âœ… Stage 2å®Œäº†æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆfinetuned_model.pthï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        print("âš ï¸ finetuned_model.pth ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é€šå¸¸ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ...")
        # é€šå¸¸ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        if len(isic_paths) > 0:
            model = pretrain_on_isic(model, train_loader_isic, val_loader_isic, epochs=5)
        
        if os.path.exists('isic_pretrained_model.pth'):
            model.load_state_dict(torch.load('isic_pretrained_model.pth', map_location=device))
            print("âœ… ISICãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ")
        
        model = finetune_on_user_data(model, train_loader_user, train_loader_user, epochs=10)
    
    # 4. SKãƒ‡ãƒ¼ã‚¿ï¼ˆè‰¯æ€§ï¼‰ã®æº–å‚™
    sk_train_paths, sk_train_labels, sk_test_paths, sk_test_labels, sk_test_diseases = load_sk_data_only()
    
    if len(sk_train_paths) > 0:
        # SKãƒ‡ãƒ¼ã‚¿ã®DataLoader
        sk_train_dataset = DermoscopyDataset(sk_train_paths, sk_train_labels, get_transforms(True))
        sk_train_loader = DataLoader(sk_train_dataset, batch_size=16, shuffle=True, num_workers=2)
        
        # 5. Stage 3: SKãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
        model = finetune_with_sk_data(model, sk_train_loader, epochs=5)
        
        # 6. æ‹¡å¼µãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ‚ªæ€§ + SKè‰¯æ€§ï¼‰
        print("\nğŸ“Š æ‹¡å¼µãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡...")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        combined_test_paths = user_test_paths + sk_test_paths
        combined_test_labels = user_test_labels + sk_test_labels
        combined_test_diseases = test_diseases + sk_test_diseases
        
        # æ‹¡å¼µãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataLoader
        combined_test_dataset = DermoscopyDataset(combined_test_paths, combined_test_labels, get_transforms(False))
        combined_test_loader = DataLoader(combined_test_dataset, batch_size=16, shuffle=False, num_workers=2)
        
        # è©•ä¾¡å®Ÿè¡Œ
        cm, disease_results = evaluate_on_test_data(
            model, combined_test_loader, combined_test_labels, combined_test_diseases
        )
        
        # 7. çµæœã®å¯è¦–åŒ–
        plot_results(cm, disease_results)
        
        print("\nğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒãƒ©ãƒ³ã‚¹æ”¹å–„çµæœ:")
        print(f"   æ‚ªæ€§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len([l for l in combined_test_labels if l == 1])}æš")
        print(f"   è‰¯æ€§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len([l for l in combined_test_labels if l == 0])}æš")
        
    else:
        print("âš ï¸ SKãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¾“æ¥ã®è©•ä¾¡ã‚’å®Ÿè¡Œ...")
        # å¾“æ¥ã®è©•ä¾¡
        cm, disease_results = evaluate_on_test_data(
            model, test_loader_user, user_test_labels, test_diseases
        )
        plot_results(cm, disease_results)
    
    print("\nâœ… 3æ®µéšãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
    print("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«:")
    print("   â€¢ isic_pretrained_model.pth: Stage 1å®Œäº†ï¼ˆISICã®ã¿ï¼‰")
    print("   â€¢ finetuned_model.pth: Stage 2å®Œäº†ï¼ˆæ‚ªæ€§ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼‰")
    print("   â€¢ balanced_finetuned_model.pth: Stage 3å®Œäº†ï¼ˆè‰¯æ€§ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰")

def run_sk_only_finetuning():
    """SKãƒ‡ãƒ¼ã‚¿ã®ã¿ã§æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    
    print("=" * 50)
    print("ğŸŒŸ SKãƒ‡ãƒ¼ã‚¿è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆç°¡æ˜“ç‰ˆï¼‰")
    print("=" * 50)
    
    # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = PretrainedModel(num_classes=2, model_type='efficientnet').to(device)
    
    if os.path.exists('finetuned_model.pth'):
        model.load_state_dict(torch.load('finetuned_model.pth', map_location=device))
        print("âœ… æ—¢å­˜ã®finetuned_model.pthã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        print("âŒ finetuned_model.pth ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # SKãƒ‡ãƒ¼ã‚¿æº–å‚™
    sk_train_paths, sk_train_labels, sk_test_paths, sk_test_labels, sk_test_diseases = load_sk_data_only()
    
    if len(sk_train_paths) == 0:
        print("âŒ SKãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # SKãƒ‡ãƒ¼ã‚¿ã®DataLoader
    sk_train_dataset = DermoscopyDataset(sk_train_paths, sk_train_labels, get_transforms(True))
    sk_train_loader = DataLoader(sk_train_dataset, batch_size=16, shuffle=True, num_workers=2)
    
    # SKãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    model = finetune_with_sk_data(model, sk_train_loader, epochs=5)
    
    print("\nâœ… SKãƒ‡ãƒ¼ã‚¿è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!")
    print("ğŸ“ balanced_finetuned_model.pth ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--sk-only":
        run_sk_only_finetuning()
    else:
        main()