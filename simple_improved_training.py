"""
ã‚·ãƒ³ãƒ—ãƒ«æ”¹å–„ç‰ˆéå­¦ç¿’å¯¾ç­–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
åŠ¹ç‡çš„ãªå®Ÿè£…ã§ç¢ºå®Ÿã«å®Œäº†
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.models import efficientnet_v2_s
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import confusion_matrix, roc_auc_score
import pandas as pd
import numpy as np
from PIL import Image
import os
import glob
import copy

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

class ImprovedModel(nn.Module):
    """æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«ï¼ˆéå­¦ç¿’å¯¾ç­–ï¼‰"""
    
    def __init__(self, num_classes=2):
        super().__init__()
        self.backbone = efficientnet_v2_s(weights='IMAGENET1K_V1')
        num_features = self.backbone.classifier[1].in_features
        
        # å¼·åŒ–ã•ã‚ŒãŸåˆ†é¡ãƒ˜ãƒƒãƒ‰ï¼ˆBatchNormé™¤å»ã§ãƒãƒƒãƒã‚µã‚¤ã‚ºå•é¡Œå›é¿ï¼‰
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(0.5),  # é«˜ã„Dropoutç‡
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )
        
        # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®ä¸€éƒ¨ã‚’å‡çµ
        self._freeze_layers()
    
    def _freeze_layers(self):
        """åˆæœŸå±¤ã‚’å‡çµã—ã¦éå­¦ç¿’ã‚’é˜²æ­¢"""
        for i, (name, param) in enumerate(self.backbone.features.named_parameters()):
            if i < 20:  # æœ€åˆã®20å±¤ã‚’å‡çµ
                param.requires_grad = False
    
    def forward(self, x):
        return self.backbone(x)

class ImprovedDataset(Dataset):
    """æ”¹å–„ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå¼·åŠ›ãªãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼‰"""
    
    def __init__(self, image_paths, labels, is_training=True):
        self.image_paths = image_paths
        self.labels = labels
        
        if is_training:
            # è¨“ç·´æ™‚ï¼šå¼·åŠ›ãªãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
            self.transform = transforms.Compose([
                transforms.Resize(256),
                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.4),
                transforms.RandomRotation(degrees=30),
                transforms.ColorJitter(
                    brightness=0.3,
                    contrast=0.3,
                    saturation=0.3,
                    hue=0.1
                ),
                transforms.RandomAffine(
                    degrees=0,
                    translate=(0.1, 0.1),
                    scale=(0.9, 1.1)
                ),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
                transforms.RandomErasing(p=0.2, scale=(0.02, 0.1))
            ])
        else:
            # æ¤œè¨¼æ™‚ï¼šåŸºæœ¬å‰å‡¦ç†ã®ã¿
            self.transform = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        try:
            image = Image.open(self.image_paths[idx]).convert('RGB')
            image = self.transform(image)
            label = self.labels[idx]
            return image, label
        except Exception as e:
            print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {self.image_paths[idx]}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿è¿”ã™
            dummy_image = torch.zeros(3, 224, 224)
            return dummy_image, self.labels[idx]

def load_balanced_subset():
    """ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã‚µãƒ–ã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    
    # HAM10000ãƒ‡ãƒ¼ã‚¿ï¼ˆå°ã•ãªã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
    metadata_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/ham10000_data/HAM10000_metadata.csv"
    df = pd.read_csv(metadata_path)
    
    # ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    image_dirs = [
        "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/ham10000_data/HAM10000_images_part_1",
        "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/ham10000_data/HAM10000_images_part_2"
    ]
    
    # ã‚¯ãƒ©ã‚¹å®šç¾©
    benign_classes = ['bkl', 'df', 'nv', 'vasc']
    malignant_classes = ['akiec', 'bcc', 'mel']
    
    # HAM10000ãƒ‡ãƒ¼ã‚¿åé›†
    ham_benign_paths = []
    ham_malignant_paths = []
    
    for _, row in df.iterrows():
        image_id = row['image_id']
        diagnosis = row['dx']
        
        # ç”»åƒãƒ‘ã‚¹æ¢ç´¢
        image_path = None
        for img_dir in image_dirs:
            potential_path = os.path.join(img_dir, f"{image_id}.jpg")
            if os.path.exists(potential_path):
                image_path = potential_path
                break
        
        if image_path:
            if diagnosis in benign_classes:
                ham_benign_paths.append(image_path)
            elif diagnosis in malignant_classes:
                ham_malignant_paths.append(image_path)
    
    # åŠ¹ç‡åŒ–ã®ãŸã‚å„ã‚¯ãƒ©ã‚¹800æšã«åˆ¶é™
    np.random.seed(42)
    if len(ham_benign_paths) > 800:
        ham_benign_paths = np.random.choice(ham_benign_paths, 800, replace=False).tolist()
    if len(ham_malignant_paths) > 800:
        ham_malignant_paths = np.random.choice(ham_malignant_paths, 800, replace=False).tolist()
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
    user_malignant_paths = []
    for disease in ['AK', 'BCC', 'Bowenç—…', 'MM']:
        disease_dir = f"/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/{disease}"
        if os.path.exists(disease_dir):
            images = glob.glob(os.path.join(disease_dir, "*.jpg")) + \
                    glob.glob(os.path.join(disease_dir, "*.JPG"))
            user_malignant_paths.extend(images)
    
    user_benign_paths = []
    sk_dir = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/SK"
    if os.path.exists(sk_dir):
        user_benign_paths = glob.glob(os.path.join(sk_dir, "*.jpg")) + \
                           glob.glob(os.path.join(sk_dir, "*.JPG"))
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    all_benign = ham_benign_paths + user_benign_paths
    all_malignant = ham_malignant_paths + user_malignant_paths
    
    # ã•ã‚‰ã«ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
    min_class_size = min(len(all_benign), len(all_malignant))
    if len(all_benign) > min_class_size:
        all_benign = np.random.choice(all_benign, min_class_size, replace=False).tolist()
    if len(all_malignant) > min_class_size:
        all_malignant = np.random.choice(all_malignant, min_class_size, replace=False).tolist()
    
    # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    all_paths = all_benign + all_malignant
    all_labels = [0] * len(all_benign) + [1] * len(all_malignant)
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    combined = list(zip(all_paths, all_labels))
    np.random.shuffle(combined)
    all_paths, all_labels = zip(*combined)
    all_paths, all_labels = list(all_paths), list(all_labels)
    
    print(f"âœ… ãƒãƒ©ãƒ³ã‚¹èª¿æ•´å®Œäº†: {len(all_paths)}æš")
    print(f"   è‰¯æ€§: {all_labels.count(0)}æš")
    print(f"   æ‚ªæ€§: {all_labels.count(1)}æš")
    
    return all_paths, all_labels

class EarlyStopping:
    """Early Stoppingå®Ÿè£…"""
    
    def __init__(self, patience=5, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_score, model):
        if self.best_score is None:
            self.best_score = val_score
            self.best_weights = copy.deepcopy(model.state_dict())
        elif val_score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_score = val_score
            self.counter = 0
            self.best_weights = copy.deepcopy(model.state_dict())
        return False

def train_improved_model():
    """æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    print("\nğŸš€ æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
    print("-" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    all_paths, all_labels = load_balanced_subset()
    
    # è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
    split_idx = int(0.8 * len(all_paths))
    train_paths = all_paths[:split_idx]
    train_labels = all_labels[:split_idx]
    val_paths = all_paths[split_idx:]
    val_labels = all_labels[split_idx:]
    
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_paths)}æš (è‰¯æ€§:{train_labels.count(0)}, æ‚ªæ€§:{train_labels.count(1)})")
    print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_paths)}æš (è‰¯æ€§:{val_labels.count(0)}, æ‚ªæ€§:{val_labels.count(1)})")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    train_dataset = ImprovedDataset(train_paths, train_labels, is_training=True)
    val_dataset = ImprovedDataset(val_paths, val_labels, is_training=False)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = ImprovedModel(num_classes=2).to(device)
    
    # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—
    class_counts = [train_labels.count(0), train_labels.count(1)]
    class_weights = [len(train_labels) / (2 * count) for count in class_counts]
    class_weights_tensor = torch.FloatTensor(class_weights).to(device)
    
    print(f"ã‚¯ãƒ©ã‚¹é‡ã¿: è‰¯æ€§={class_weights[0]:.2f}, æ‚ªæ€§={class_weights[1]:.2f}")
    
    # æå¤±é–¢æ•°ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.02)  # å¼·ã„Weight Decay
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)
    
    # Early Stopping
    early_stopping = EarlyStopping(patience=7, min_delta=0.001)
    
    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    best_auc = 0
    print("\nğŸ”„ è¨“ç·´é–‹å§‹...")
    
    for epoch in range(25):  # æœ€å¤§25ã‚¨ãƒãƒƒã‚¯
        # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1)
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
        
        train_acc = 100. * train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)
        
        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_preds = []
        val_targets = []
        val_probs = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                pred = output.argmax(dim=1)
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
                
                # AUCè¨ˆç®—ç”¨
                probs = torch.softmax(output, dim=1)
                val_probs.extend(probs[:, 1].cpu().numpy())
                val_preds.extend(pred.cpu().numpy())
                val_targets.extend(target.cpu().numpy())
        
        val_acc = 100. * val_correct / val_total
        avg_val_loss = val_loss / len(val_loader)
        
        # AUCè¨ˆç®—
        try:
            val_auc = roc_auc_score(val_targets, val_probs)
        except:
            val_auc = 0.5
        
        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        scheduler.step(val_auc)
        current_lr = optimizer.param_groups[0]['lr']
        
        # çµæœè¡¨ç¤º
        print(f"Epoch {epoch+1:2d}: "
              f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.1f}% | "
              f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.1f}%, Val AUC: {val_auc:.3f} | "
              f"LR: {current_lr:.2e}")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if val_auc > best_auc:
            best_auc = val_auc
            best_model_state = copy.deepcopy(model.state_dict())
            print(f"   âœ… æ–°ã—ã„ãƒ™ã‚¹ãƒˆAUC: {best_auc:.3f}")
        
        # Early Stopping ãƒã‚§ãƒƒã‚¯
        if early_stopping(val_auc, model):
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    print(f"\nâœ… è¨“ç·´å®Œäº†! æœ€è‰¯AUC: {best_auc:.3f}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/quick_improved_model.pth"
    torch.save({
        'model_state_dict': best_model_state,
        'best_auc': best_auc,
        'final_val_targets': val_targets,
        'final_val_preds': val_preds,
        'final_val_probs': val_probs,
        'model_type': 'improved'
    }, model_path)
    
    print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
    
    # æœ€çµ‚è©•ä¾¡
    if len(set(val_targets)) > 1:  # ä¸¡ã‚¯ãƒ©ã‚¹ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
        cm = confusion_matrix(val_targets, val_preds)
        tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, sum(val_targets))
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        print(f"\nğŸ“Š æœ€çµ‚è©•ä¾¡çµæœ:")
        print(f"æ„Ÿåº¦ (Sensitivity): {sensitivity:.1%}")
        print(f"ç‰¹ç•°åº¦ (Specificity): {specificity:.1%}")
        print(f"AUC: {best_auc:.3f}")
        print(f"æ··åŒè¡Œåˆ—:")
        print(f"  å®Ÿéš›\\äºˆæ¸¬   è‰¯æ€§  æ‚ªæ€§")
        print(f"  è‰¯æ€§        {tn:4d}  {fp:4d}")
        print(f"  æ‚ªæ€§        {fn:4d}  {tp:4d}")
    
    return model_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¬ ã‚·ãƒ³ãƒ—ãƒ«æ”¹å–„ç‰ˆãƒ€ãƒ¼ãƒ¢ã‚¹ã‚³ãƒ”ãƒ¼åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ")
    print("   åŠ¹ç‡çš„ãªéå­¦ç¿’å¯¾ç­–å®Ÿè£…")
    print("="*60)
    
    try:
        model_path = train_improved_model()
        
        print(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"   python3 predict_quick_improved.py")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()