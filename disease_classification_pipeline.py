"""
ç–¾æ‚£åˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
HAM10000äº‹å‰å­¦ç¿’ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â†’ ç–¾æ‚£åˆ¥è‰¯æ€§ãƒ»æ‚ªæ€§åˆ¤å®š
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.models import efficientnet_v2_s
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score
import pandas as pd
import numpy as np
from PIL import Image
import os
import glob
import copy
import json
from collections import defaultdict

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ç–¾æ‚£åˆ†é¡å®šç¾©
DISEASE_MAPPING = {
    'AK': {'type': 'malignant', 'full_name': 'Actinic Keratosis'},
    'BCC': {'type': 'malignant', 'full_name': 'Basal Cell Carcinoma'}, 
    'Bowenç—…': {'type': 'malignant', 'full_name': 'Bowen Disease'},
    'MM': {'type': 'malignant', 'full_name': 'Malignant Melanoma'},
    'SK': {'type': 'benign', 'full_name': 'Seborrheic Keratosis'}
}

class DiseaseClassificationModel(nn.Module):
    """ç–¾æ‚£åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆè‰¯æ€§ãƒ»æ‚ªæ€§åˆ†é¡ï¼‰"""
    
    def __init__(self, num_classes=2, dropout_rate=0.3):
        super().__init__()
        self.backbone = efficientnet_v2_s(weights='IMAGENET1K_V1')
        num_features = self.backbone.classifier[1].in_features
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        return self.backbone(x)

class DiseaseDataset(Dataset):
    """ç–¾æ‚£ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, image_paths, labels, disease_names, is_training=True):
        self.image_paths = image_paths
        self.labels = labels
        self.disease_names = disease_names
        
        if is_training:
            self.transform = transforms.Compose([
                transforms.Resize(256),
                transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.3),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        try:
            image = Image.open(self.image_paths[idx]).convert('RGB')
            image = self.transform(image)
            label = self.labels[idx]
            disease = self.disease_names[idx]
            return image, label, disease
        except Exception as e:
            print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {self.image_paths[idx]} - {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            dummy_image = torch.zeros(3, 224, 224)
            return dummy_image, self.labels[idx], self.disease_names[idx]

def load_ham10000_data():
    """HAM10000ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š HAM10000ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    metadata_path = "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/ham10000_data/HAM10000_metadata.csv"
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"HAM10000ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_path}")
    
    df = pd.read_csv(metadata_path)
    
    image_dirs = [
        "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/ham10000_data/HAM10000_images_part_1",
        "/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/ham10000_data/HAM10000_images_part_2"
    ]
    
    # HAM10000ã‚¯ãƒ©ã‚¹å®šç¾©
    benign_classes = ['bkl', 'df', 'nv', 'vasc']  # è‰¯æ€§
    malignant_classes = ['akiec', 'bcc', 'mel']   # æ‚ªæ€§
    
    image_paths = []
    labels = []
    diseases = []
    
    for _, row in df.iterrows():
        image_id = row['image_id']
        diagnosis = row['dx']
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢
        image_path = None
        for img_dir in image_dirs:
            potential_path = os.path.join(img_dir, f"{image_id}.jpg")
            if os.path.exists(potential_path):
                image_path = potential_path
                break
        
        if image_path and diagnosis in (benign_classes + malignant_classes):
            image_paths.append(image_path)
            # ãƒ©ãƒ™ãƒ«: 0=è‰¯æ€§, 1=æ‚ªæ€§
            labels.append(0 if diagnosis in benign_classes else 1)
            diseases.append(f"HAM_{diagnosis}")
    
    print(f"âœ… HAM10000ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(image_paths)}æš")
    print(f"   è‰¯æ€§: {labels.count(0)}æš, æ‚ªæ€§: {labels.count(1)}æš")
    
    return image_paths, labels, diseases

def load_user_disease_data():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ç–¾æ‚£ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ğŸ“Š ãƒ¦ãƒ¼ã‚¶ãƒ¼ç–¾æ‚£ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    all_paths = []
    all_labels = []
    all_diseases = []
    
    disease_stats = {}
    
    for disease, info in DISEASE_MAPPING.items():
        disease_dir = f"/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/{disease}"
        if not os.path.exists(disease_dir):
            print(f"âš ï¸ {disease}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {disease_dir}")
            continue
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        image_files = glob.glob(os.path.join(disease_dir, "*.jpg")) + \
                     glob.glob(os.path.join(disease_dir, "*.JPG")) + \
                     glob.glob(os.path.join(disease_dir, "*.jpeg"))
        
        disease_stats[disease] = {
            'count': len(image_files),
            'type': info['type'],
            'full_name': info['full_name']
        }
        
        for img_path in image_files:
            all_paths.append(img_path)
            all_labels.append(0 if info['type'] == 'benign' else 1)
            all_diseases.append(disease)
    
    print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ç–¾æ‚£ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(all_paths)}æš")
    for disease, stats in disease_stats.items():
        print(f"   {disease} ({stats['full_name']}): {stats['count']}æš [{stats['type']}]")
    
    return all_paths, all_labels, all_diseases, disease_stats

def pretrain_on_ham10000(model, ham_paths, ham_labels, ham_diseases, epochs=10):
    """HAM10000ã§ã®äº‹å‰å­¦ç¿’"""
    print(f"\nğŸ”„ HAM10000äº‹å‰å­¦ç¿’é–‹å§‹ ({epochs}ã‚¨ãƒãƒƒã‚¯)")
    print("-" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²
    np.random.seed(42)
    indices = np.random.permutation(len(ham_paths))
    split_idx = int(0.8 * len(indices))
    
    train_indices = indices[:split_idx]
    val_indices = indices[split_idx:]
    
    train_paths = [ham_paths[i] for i in train_indices]
    train_labels = [ham_labels[i] for i in train_indices]
    train_diseases = [ham_diseases[i] for i in train_indices]
    
    val_paths = [ham_paths[i] for i in val_indices]
    val_labels = [ham_labels[i] for i in val_indices]
    val_diseases = [ham_diseases[i] for i in val_indices]
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    train_dataset = DiseaseDataset(train_paths, train_labels, train_diseases, is_training=True)
    val_dataset = DiseaseDataset(val_paths, val_labels, val_diseases, is_training=False)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)
    
    # æœ€é©åŒ–è¨­å®š
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)
    
    best_auc = 0
    best_state = None
    
    for epoch in range(epochs):
        # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target, _) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1)
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
        
        train_acc = 100. * train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)
        
        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_preds = []
        val_targets = []
        val_probs = []
        
        with torch.no_grad():
            for data, target, _ in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                pred = output.argmax(dim=1)
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
                
                # AUCè¨ˆç®—ç”¨
                probs = torch.softmax(output, dim=1)
                val_probs.extend(probs[:, 1].cpu().numpy())
                val_preds.extend(pred.cpu().numpy())
                val_targets.extend(target.cpu().numpy())
        
        val_acc = 100. * val_correct / val_total
        avg_val_loss = val_loss / len(val_loader)
        
        # AUCè¨ˆç®—
        try:
            val_auc = roc_auc_score(val_targets, val_probs)
        except:
            val_auc = 0.5
        
        scheduler.step(val_auc)
        
        print(f"Epoch {epoch+1:2d}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.1f}% | "
              f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.1f}%, Val AUC: {val_auc:.3f}")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if val_auc > best_auc:
            best_auc = val_auc
            best_state = copy.deepcopy(model.state_dict())
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«å¾©å…ƒ
    model.load_state_dict(best_state)
    
    print(f"âœ… HAM10000äº‹å‰å­¦ç¿’å®Œäº†! æœ€è‰¯AUC: {best_auc:.3f}")
    return model

def finetune_on_user_data(model, user_paths, user_labels, user_diseases, epochs=15):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
    print(f"\nğŸ”„ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ({epochs}ã‚¨ãƒãƒƒã‚¯)")
    print("-" * 50)
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ã«ä½¿ç”¨ï¼ˆå°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãŸã‚ï¼‰
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    train_dataset = DiseaseDataset(user_paths, user_labels, user_diseases, is_training=True)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
    
    # ã‚¯ãƒ©ã‚¹é‡ã¿èª¿æ•´
    class_counts = [user_labels.count(0), user_labels.count(1)]
    class_weights = [len(user_labels) / (2 * count) for count in class_counts]
    class_weights_tensor = torch.FloatTensor(class_weights).to(device)
    
    print(f"ã‚¯ãƒ©ã‚¹é‡ã¿: è‰¯æ€§={class_weights[0]:.2f}, æ‚ªæ€§={class_weights[1]:.2f}")
    
    # æœ€é©åŒ–è¨­å®šï¼ˆä½ã„å­¦ç¿’ç‡ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target, _) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1)
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
        
        train_acc = 100. * train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)
        
        print(f"Epoch {epoch+1:2d}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.1f}%")
    
    print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!")
    return model

def evaluate_disease_classification(model, user_paths, user_labels, user_diseases, disease_stats):
    """ç–¾æ‚£åˆ†é¡æ€§èƒ½è©•ä¾¡"""
    print(f"\nğŸ“Š ç–¾æ‚£åˆ†é¡æ€§èƒ½è©•ä¾¡")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    eval_dataset = DiseaseDataset(user_paths, user_labels, user_diseases, is_training=False)
    eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=2)
    
    model.eval()
    all_preds = []
    all_targets = []
    all_probs = []
    all_diseases = []
    
    with torch.no_grad():
        for data, target, disease in eval_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            
            pred = output.argmax(dim=1)
            probs = torch.softmax(output, dim=1)
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())  # æ‚ªæ€§ã®ç¢ºç‡
            all_diseases.extend(disease)
    
    # å…¨ä½“è©•ä¾¡
    overall_accuracy = accuracy_score(all_targets, all_preds)
    overall_auc = roc_auc_score(all_targets, all_probs)
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(all_targets, all_preds)
    tn, fp, fn, tp = cm.ravel()
    
    # æ„Ÿåº¦ãƒ»ç‰¹ç•°åº¦è¨ˆç®—
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # æ‚ªæ€§ã‚’æ­£ã—ãæ¤œå‡º
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # è‰¯æ€§ã‚’æ­£ã—ãè­˜åˆ¥
    
    print(f"ğŸ¯ å…¨ä½“æ€§èƒ½:")
    print(f"   ç²¾åº¦ (Accuracy): {overall_accuracy:.1%}")
    print(f"   AUC: {overall_auc:.3f}")
    print(f"   æ„Ÿåº¦ (Sensitivity): {sensitivity:.1%}")
    print(f"   ç‰¹ç•°åº¦ (Specificity): {specificity:.1%}")
    
    print(f"\nğŸ“‹ æ··åŒè¡Œåˆ—:")
    print(f"   å®Ÿéš›\\\\äºˆæ¸¬   è‰¯æ€§  æ‚ªæ€§")
    print(f"   è‰¯æ€§        {tn:4d}  {fp:4d}")
    print(f"   æ‚ªæ€§        {fn:4d}  {tp:4d}")
    
    # ç–¾æ‚£åˆ¥è©•ä¾¡
    print(f"\nğŸ”¬ ç–¾æ‚£åˆ¥è©³ç´°è©•ä¾¡:")
    print("-" * 60)
    
    disease_results = {}
    
    for disease in DISEASE_MAPPING.keys():
        if disease not in disease_stats:
            continue
            
        # ç–¾æ‚£åˆ¥ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        disease_indices = [i for i, d in enumerate(all_diseases) if d == disease]
        if not disease_indices:
            continue
            
        disease_targets = [all_targets[i] for i in disease_indices]
        disease_preds = [all_preds[i] for i in disease_indices]
        disease_probs = [all_probs[i] for i in disease_indices]
        
        # ç–¾æ‚£åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        disease_accuracy = accuracy_score(disease_targets, disease_preds)
        correct_count = sum(1 for t, p in zip(disease_targets, disease_preds) if t == p)
        total_count = len(disease_targets)
        
        # å¹³å‡ç¢ºä¿¡åº¦
        avg_confidence = np.mean([max(1-p, p) for p in disease_probs])
        
        disease_results[disease] = {
            'accuracy': disease_accuracy,
            'correct': correct_count,
            'total': total_count,
            'confidence': avg_confidence,
            'true_type': DISEASE_MAPPING[disease]['type']
        }
        
        print(f"{disease} ({DISEASE_MAPPING[disease]['full_name']}):")
        print(f"   æ­£è§£ç‡: {correct_count}/{total_count} ({disease_accuracy:.1%})")
        print(f"   å¹³å‡ç¢ºä¿¡åº¦: {avg_confidence:.1%}")
        print(f"   å®Ÿéš›ã®ã‚¿ã‚¤ãƒ—: {DISEASE_MAPPING[disease]['type']}")
    
    # è‰¯æ€§ãƒ»æ‚ªæ€§åˆ¥è©•ä¾¡
    print(f"\nğŸ“ˆ è‰¯æ€§ãƒ»æ‚ªæ€§åˆ¥è©•ä¾¡:")
    print("-" * 40)
    
    benign_diseases = [d for d in DISEASE_MAPPING.keys() if DISEASE_MAPPING[d]['type'] == 'benign']
    malignant_diseases = [d for d in DISEASE_MAPPING.keys() if DISEASE_MAPPING[d]['type'] == 'malignant']
    
    for disease_type, diseases in [('è‰¯æ€§', benign_diseases), ('æ‚ªæ€§', malignant_diseases)]:
        type_indices = [i for i, d in enumerate(all_diseases) if d in diseases]
        if type_indices:
            type_targets = [all_targets[i] for i in type_indices]
            type_preds = [all_preds[i] for i in type_indices]
            type_accuracy = accuracy_score(type_targets, type_preds)
            correct = sum(1 for t, p in zip(type_targets, type_preds) if t == p)
            total = len(type_targets)
            
            print(f"{disease_type}ç–¾æ‚£:")
            print(f"   æ­£è§£ç‡: {correct}/{total} ({type_accuracy:.1%})")
    
    # çµæœä¿å­˜ï¼ˆå‹å¤‰æ›ã—ã¦JSONå¯¾å¿œï¼‰
    def convert_to_json_serializable(obj):
        """NumPyå‹ã‚’JSONå¯¾å¿œå‹ã«å¤‰æ›"""
        if isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        else:
            return obj
    
    results = {
        'overall': {
            'accuracy': float(overall_accuracy),
            'auc': float(overall_auc),
            'sensitivity': float(sensitivity),
            'specificity': float(specificity),
            'confusion_matrix': cm.tolist()
        },
        'diseases': convert_to_json_serializable(disease_results)
    }
    
    with open('/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/disease_classification_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è©•ä¾¡çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: disease_classification_results.json")
    
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¬ ç–¾æ‚£åˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("   HAM10000äº‹å‰å­¦ç¿’ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â†’ ç–¾æ‚£åˆ¥è©•ä¾¡")
    print("=" * 80)
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        ham_paths, ham_labels, ham_diseases = load_ham10000_data()
        user_paths, user_labels, user_diseases, disease_stats = load_user_disease_data()
        
        # 2. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model = DiseaseClassificationModel(num_classes=2, dropout_rate=0.3).to(device)
        
        # 3. HAM10000äº‹å‰å­¦ç¿’
        model = pretrain_on_ham10000(model, ham_paths, ham_labels, ham_diseases, epochs=10)
        
        # äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        torch.save({
            'model_state_dict': model.state_dict(),
            'disease_mapping': DISEASE_MAPPING
        }, '/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/ham10000_pretrained_disease_model.pth')
        print("ğŸ’¾ HAM10000äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        
        # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
        model = finetune_on_user_data(model, user_paths, user_labels, user_diseases, epochs=15)
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        torch.save({
            'model_state_dict': model.state_dict(),
            'disease_mapping': DISEASE_MAPPING
        }, '/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/disease_classification_model.pth')
        print("ğŸ’¾ ç–¾æ‚£åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        
        # 5. ç–¾æ‚£åˆ†é¡æ€§èƒ½è©•ä¾¡
        results = evaluate_disease_classification(model, user_paths, user_labels, user_diseases, disease_stats)
        
        print(f"\nğŸ‰ ç–¾æ‚£åˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
        print(f"ğŸ¯ å…¨ä½“ç²¾åº¦: {results['overall']['accuracy']:.1%}")
        print(f"ğŸ¯ æ„Ÿåº¦: {results['overall']['sensitivity']:.1%}")
        print(f"ğŸ¯ ç‰¹ç•°åº¦: {results['overall']['specificity']:.1%}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()