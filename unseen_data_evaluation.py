"""
æœªè¦‹ãƒ‡ãƒ¼ã‚¿è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
å®Œå…¨ã«å­¦ç¿’ã«ä½¿ç”¨ã—ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.models import efficientnet_v2_s, resnet50
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve
import numpy as np
from PIL import Image
import os
import glob
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')

# ç–¾æ‚£åˆ†é¡å®šç¾©
DISEASE_MAPPING = {
    'AK': {'type': 'malignant', 'full_name': 'Actinic Keratosis'},
    'BCC': {'type': 'malignant', 'full_name': 'Basal Cell Carcinoma'}, 
    'Bowenç—…': {'type': 'malignant', 'full_name': 'Bowen Disease'},
    'MM': {'type': 'malignant', 'full_name': 'Malignant Melanoma'},
    'SK': {'type': 'benign', 'full_name': 'Seborrheic Keratosis'}
}

class UnseenDataEvaluator:
    """æœªè¦‹ãƒ‡ãƒ¼ã‚¿è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.results = {}
        
    def collect_all_data(self, base_path='/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢'):
        """å…¨ãƒ‡ãƒ¼ã‚¿åé›†ã¨åˆ†å‰²"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†ã¨æœªè¦‹ãƒ‡ãƒ¼ã‚¿åˆ†é›¢")
        print("=" * 60)
        
        all_data = {
            'paths': [],
            'labels': [],
            'diseases': [],
            'patient_ids': []
        }
        
        for disease, info in DISEASE_MAPPING.items():
            disease_dir = os.path.join(base_path, disease)
            if not os.path.exists(disease_dir):
                continue
            
            patterns = ['*.jpg', '*.JPG', '*.jpeg', '*.png']
            image_paths = []
            for pattern in patterns:
                image_paths.extend(glob.glob(os.path.join(disease_dir, pattern)))
            
            label = 1 if info['type'] == 'malignant' else 0
            
            # å„ç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
            for i, img_path in enumerate(image_paths):
                all_data['paths'].append(img_path)
                all_data['labels'].append(label)
                all_data['diseases'].append(disease)
                # æ‚£è€…IDã‚’ç”Ÿæˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åãƒ™ãƒ¼ã‚¹ï¼‰
                filename = os.path.basename(img_path)
                patient_id = f"{disease}_{filename.split('.')[0]}"
                all_data['patient_ids'].append(patient_id)
            
            print(f"   {disease}: {len(image_paths)}æš ({'æ‚ªæ€§' if label == 1 else 'è‰¯æ€§'})")
        
        total_images = len(all_data['paths'])
        print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ç·æ•°: {total_images}æš")
        
        return all_data
    
    def create_unseen_test_set(self, all_data, test_ratio=0.2):
        """å®Œå…¨ã«ç‹¬ç«‹ã—ãŸãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ä½œæˆ"""
        print(f"\nğŸ”„ æœªè¦‹ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆä½œæˆï¼ˆ{test_ratio:.0%}ã‚’ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆï¼‰")
        print("-" * 40)
        
        # ç–¾æ‚£ã”ã¨ã«å±¤åŒ–ã—ã¦ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ä½œæˆ
        train_indices = []
        test_indices = []
        
        for disease in DISEASE_MAPPING.keys():
            # è©²å½“ç–¾æ‚£ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            disease_indices = [i for i, d in enumerate(all_data['diseases']) if d == disease]
            
            if len(disease_indices) < 2:
                # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹å ´åˆã¯å…¨ã¦è¨“ç·´ç”¨ã«
                train_indices.extend(disease_indices)
                continue
            
            # å±¤åŒ–åˆ†å‰²
            n_test = max(1, int(len(disease_indices) * test_ratio))
            np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
            np.random.shuffle(disease_indices)
            
            test_indices.extend(disease_indices[:n_test])
            train_indices.extend(disease_indices[n_test:])
            
            print(f"   {disease}: è¨“ç·´ {len(disease_indices) - n_test}æš, ãƒ†ã‚¹ãƒˆ {n_test}æš")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        train_data = {
            'paths': [all_data['paths'][i] for i in train_indices],
            'labels': [all_data['labels'][i] for i in train_indices],
            'diseases': [all_data['diseases'][i] for i in train_indices],
            'patient_ids': [all_data['patient_ids'][i] for i in train_indices]
        }
        
        test_data = {
            'paths': [all_data['paths'][i] for i in test_indices],
            'labels': [all_data['labels'][i] for i in test_indices],
            'diseases': [all_data['diseases'][i] for i in test_indices],
            'patient_ids': [all_data['patient_ids'][i] for i in test_indices]
        }
        
        print(f"\nğŸ“Š åˆ†å‰²çµæœ:")
        print(f"   è¨“ç·´ã‚»ãƒƒãƒˆ: {len(train_data['paths'])}æš")
        print(f"   ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ: {len(test_data['paths'])}æšï¼ˆå®Œå…¨ã«æœªè¦‹ï¼‰")
        
        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®åˆ†å¸ƒç¢ºèª
        test_malignant = sum(test_data['labels'])
        test_benign = len(test_data['labels']) - test_malignant
        print(f"   ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆå†…è¨³: æ‚ªæ€§ {test_malignant}æš, è‰¯æ€§ {test_benign}æš")
        
        return train_data, test_data
    
    def train_model_on_subset(self, train_data, model_type='efficientnet'):
        """è¨“ç·´ã‚»ãƒƒãƒˆã®ã¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print(f"\nğŸš€ {model_type.upper()} è¨“ç·´ï¼ˆè¨“ç·´ã‚»ãƒƒãƒˆã®ã¿ä½¿ç”¨ï¼‰")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        if model_type == 'efficientnet':
            model = self.create_efficientnet()
        else:
            model = self.create_resnet()
        
        model = model.to(device)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_dataset = SimpleDataset(train_data['paths'], train_data['labels'])
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # è¨“ç·´
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=1e-4)
        
        model.train()
        for epoch in range(10):  # ç°¡ç•¥åŒ–ã®ãŸã‚10ã‚¨ãƒãƒƒã‚¯
            total_loss = 0
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 3 == 0:
                print(f"   Epoch {epoch+1}: Loss {total_loss/len(train_loader):.4f}")
        
        print(f"âœ… è¨“ç·´å®Œäº†")
        return model
    
    def create_efficientnet(self):
        """EfficientNetãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        model = efficientnet_v2_s(weights='IMAGENET1K_V1')
        num_features = model.classifier[1].in_features
        model.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 2)
        )
        return model
    
    def create_resnet(self):
        """ResNetãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        model = resnet50(weights='IMAGENET1K_V1')
        num_features = model.fc.in_features
        model.fc = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 2)
        )
        return model
    
    def evaluate_on_unseen_data(self, model, test_data):
        """æœªè¦‹ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡"""
        print("\nğŸ§ª æœªè¦‹ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡")
        
        model.eval()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        test_dataset = SimpleDataset(test_data['paths'], test_data['labels'])
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        all_probs = []
        all_labels = []
        all_preds = []
        
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(device)
                outputs = model(images)
                probs = torch.softmax(outputs, dim=1)[:, 1]  # æ‚ªæ€§ç¢ºç‡
                preds = (probs > 0.5).float()
                
                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(labels.numpy())
                all_preds.extend(preds.cpu().numpy())
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        all_labels = np.array(all_labels)
        all_preds = np.array(all_preds)
        all_probs = np.array(all_probs)
        
        # æ··åŒè¡Œåˆ—
        tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()
        
        # å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        accuracy = accuracy_score(all_labels, all_preds)
        precision = precision_score(all_labels, all_preds, zero_division=0)
        recall = recall_score(all_labels, all_preds, zero_division=0)  # æ„Ÿåº¦
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = f1_score(all_labels, all_preds, zero_division=0)
        
        # AUCï¼ˆä¸¡ã‚¯ãƒ©ã‚¹ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        if len(np.unique(all_labels)) > 1:
            auc = roc_auc_score(all_labels, all_probs)
        else:
            auc = None
        
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall_sensitivity': recall,
            'specificity': specificity,
            'f1_score': f1,
            'auc': auc,
            'confusion_matrix': {
                'TP': int(tp), 'TN': int(tn), 
                'FP': int(fp), 'FN': int(fn)
            },
            'total_samples': len(all_labels),
            'malignant_samples': int(sum(all_labels)),
            'benign_samples': len(all_labels) - int(sum(all_labels))
        }
        
        return results
    
    def generate_report(self, results):
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ æœªè¦‹ãƒ‡ãƒ¼ã‚¿è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        
        print("\nğŸ¯ æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå®Œå…¨ã«æœªè¦‹ã®ãƒ‡ãƒ¼ã‚¿ï¼‰:")
        print("-" * 50)
        print(f"   ç²¾åº¦ (Accuracy): {results['accuracy']:.3f}")
        print(f"   é©åˆç‡ (Precision): {results['precision']:.3f}")
        print(f"   æ„Ÿåº¦ (Sensitivity/Recall): {results['recall_sensitivity']:.3f}")
        print(f"   ç‰¹ç•°åº¦ (Specificity): {results['specificity']:.3f}")
        print(f"   F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.3f}")
        if results['auc'] is not None:
            print(f"   AUC: {results['auc']:.3f}")
        
        print(f"\nğŸ“Š æ··åŒè¡Œåˆ—:")
        cm = results['confusion_matrix']
        print(f"   çœŸé™½æ€§ (TP): {cm['TP']} | å½é™½æ€§ (FP): {cm['FP']}")
        print(f"   å½é™°æ€§ (FN): {cm['FN']} | çœŸé™°æ€§ (TN): {cm['TN']}")
        
        print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
        print(f"   ãƒ†ã‚¹ãƒˆç·æ•°: {results['total_samples']}æš")
        print(f"   æ‚ªæ€§: {results['malignant_samples']}æš")
        print(f"   è‰¯æ€§: {results['benign_samples']}æš")
        
        # è‡¨åºŠçš„è§£é‡ˆ
        print(f"\nğŸ¥ è‡¨åºŠçš„è§£é‡ˆ:")
        if results['recall_sensitivity'] >= 0.9:
            print("   âœ… é«˜ã„æ„Ÿåº¦ - æ‚ªæ€§ã®è¦‹é€ƒã—ãŒå°‘ãªã„")
        else:
            print("   âš ï¸ æ„Ÿåº¦ã«æ”¹å–„ã®ä½™åœ°ã‚ã‚Š")
        
        if results['specificity'] >= 0.9:
            print("   âœ… é«˜ã„ç‰¹ç•°åº¦ - è‰¯æ€§ã‚’æ­£ã—ãåˆ¤å®š")
        else:
            print("   âš ï¸ ç‰¹ç•°åº¦ã«æ”¹å–„ã®ä½™åœ°ã‚ã‚Š")
        
        return results

class SimpleDataset(Dataset):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, paths, labels):
        self.paths = paths
        self.labels = labels
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.paths)
    
    def __getitem__(self, idx):
        image = Image.open(self.paths[idx]).convert('RGB')
        image = self.transform(image)
        return image, self.labels[idx]

def check_for_new_images(base_path='/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢'):
    """æ–°ã—ã„ç”»åƒã®ç¢ºèª"""
    print("\nğŸ” æ–°è¦ç”»åƒã®ç¢ºèª")
    print("-" * 40)
    
    # ç‰¹åˆ¥ãªãƒ†ã‚¹ãƒˆç”»åƒã®ç¢ºèª
    special_images = []
    
    # test.JPG
    test_jpg = os.path.join(base_path, 'test.JPG')
    if os.path.exists(test_jpg):
        special_images.append(test_jpg)
        print(f"   âœ… test.JPG ç™ºè¦‹")
    
    # images.jpeg
    images_jpeg = os.path.join(base_path, 'images.jpeg')
    if os.path.exists(images_jpeg):
        special_images.append(images_jpeg)
        print(f"   âœ… images.jpeg ç™ºè¦‹")
    
    # æ–°è¦è¿½åŠ ç”»åƒã®æ¤œç´¢
    recent_images = []
    import time
    current_time = time.time()
    
    for root, dirs, files in os.walk(base_path):
        # ç–¾æ‚£ãƒ•ã‚©ãƒ«ãƒ€ä»¥å¤–ã¯é™¤å¤–
        if any(disease in root for disease in DISEASE_MAPPING.keys()):
            continue
            
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                file_path = os.path.join(root, file)
                # æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ24æ™‚é–“ä»¥å†…ï¼‰
                if current_time - os.path.getmtime(file_path) < 86400:
                    recent_images.append(file_path)
    
    if recent_images:
        print(f"   ğŸ“ æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸç”»åƒ: {len(recent_images)}æš")
        for img in recent_images[:5]:  # æœ€åˆã®5æšè¡¨ç¤º
            print(f"      - {os.path.basename(img)}")
    
    return special_images, recent_images

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ”¬ æœªè¦‹ãƒ‡ãƒ¼ã‚¿è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    print("   å®Œå…¨ã«å­¦ç¿’ã«ä½¿ç”¨ã—ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã®çœŸã®æ€§èƒ½è©•ä¾¡")
    print("=" * 80)
    
    evaluator = UnseenDataEvaluator()
    
    # æ–°è¦ç”»åƒã®ç¢ºèª
    special_images, recent_images = check_for_new_images()
    
    if special_images or recent_images:
        print("\nğŸ’¡ è¿½åŠ ç”»åƒã‚’ä½¿ç”¨ã—ãŸè©•ä¾¡ãŒå¯èƒ½ã§ã™")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿åé›†
    all_data = evaluator.collect_all_data()
    
    # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆåˆ†å‰²ï¼ˆ80:20ï¼‰
    train_data, test_data = evaluator.create_unseen_test_set(all_data, test_ratio=0.2)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆè¨“ç·´ã‚»ãƒƒãƒˆã®ã¿ä½¿ç”¨ï¼‰
    model = evaluator.train_model_on_subset(train_data, model_type='efficientnet')
    
    # æœªè¦‹ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    results = evaluator.evaluate_on_unseen_data(model, test_data)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    final_results = evaluator.generate_report(results)
    
    # çµæœä¿å­˜
    with open('/Users/iinuma/Desktop/ãƒ€ãƒ¼ãƒ¢/unseen_data_evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜: unseen_data_evaluation_results.json")
    print(f"\nğŸ‰ æœªè¦‹ãƒ‡ãƒ¼ã‚¿è©•ä¾¡å®Œäº†ï¼")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ææ¡ˆ
    print(f"\n" + "=" * 80)
    print("ğŸ’¡ ã‚ˆã‚Šæ­£ç¢ºãªè©•ä¾¡ã®ãŸã‚ã«ï¼š")
    print("=" * 80)
    print("1. æ–°ã—ã„è¨ºæ–­ç”»åƒã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
    print("2. å„ç–¾æ‚£ã«ã¤ã10-20æšã®æ–°è¦ç”»åƒãŒã‚ã‚‹ã¨ç†æƒ³çš„ã§ã™")
    print("3. ç‰¹ã«SKã®è¿½åŠ ç”»åƒãŒã‚ã‚‹ã¨èª¤åˆ†é¡æ”¹å–„ã®æ¤œè¨¼ã«æœ‰åŠ¹ã§ã™")

if __name__ == "__main__":
    main()